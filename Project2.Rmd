---
title: "Linear Regression using a dataset with used car sales"
author: "Ioannis Vourkas"
date: "`r Sys.Date()`"
output:
  html_document: default
---

```{r setup, include=FALSE}
# Load necessary libraries
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

US Used cars dataset 
===

The original data set contains 3 million real world used cars details (from kaggle [https://www.kaggle.com/datasets/ananaymital/us-used-cars-dataset/data]). 
However, in order to process it in R, using Python, I limited it to 30,000 values and filled the NaNs in the way described in the corresponding jupyter notebook.The measured variables:

1. **`body_type`**: The general style of the car body (e.g., sedan, SUV, truck). This variable helps classify the vehicle type based on its shape and purpose.

2. **`daysonmarket`**: The number of days the car has been listed on the market. This variable can indicate how long certain car models tend to stay unsold, which might imply demand or popularity.

3. **`fuel_type`**: The type of fuel used by the car (e.g., gasoline, diesel, electric). Different fuel types can affect a car’s operating cost, environmental impact, and market demand.

4. **`has_accidents`**: A binary indicator of whether the car has been in an accident. Cars with accident histories often have lower resale values, which makes this an important feature for pricing.

5. **`highway_fuel_economy`**: The car's fuel efficiency on highways, usually measured in miles per gallon (mpg). This can be a key factor for buyers who prioritize fuel efficiency, particularly for long-distance driving.

6. **`horsepower`**: The power output of the car’s engine, measured in horsepower. Horsepower often influences a car's performance and is a major consideration for buyers focused on speed and acceleration.

7. **`is_new`**: A binary variable indicating if the car is new or used. This is crucial for differentiating pricing models and expectations, as new cars typically have a premium over used ones.

8. **`make_name`**: The manufacturer or brand of the car (e.g., Toyota, Ford). Brand is often correlated with reliability, luxury status, and price.

9. **`mileage`**: The total distance the car has traveled, typically measured in miles or kilometers. Higher mileage often decreases a car's value as it suggests more wear and tear.

10. **`price`**: The listing or sale price of the car. This is generally the target variable in pricing analyses or models for predicting car value.

11. **`seller_rating`**: The rating given to the seller, which may indicate trustworthiness or quality of service. Buyers often prefer sellers with higher ratings, as they may provide more reliable information or a better purchasing experience.

12. **`year`**: The model year of the car, indicating when it was manufactured. Older cars typically depreciate in value, while newer models may command higher prices.





---

## Scope

To effectively predict used car prices with a linear regression model, starting with Exploratory Data Analysis (EDA) is crucial. EDA helps you understand the relationships between the different factors (features) and the price, which will guide you in choosing and transforming the features for the predictive model.

<span style="color: blue">*Step 1: Reading and Understanding the Data*</span>
===
```{r cars}

cars.df <- read.csv("cleaned_used_cars_data.csv", TRUE, ",")
dim(cars.df)

```
---
```{r}

str(cars.df) #Check the structure of the dataframe
summary(cars.df) #Summary statistics for numerical variables
head(cars.df) #View the first few rows
```

## <span style="color: blue">*Step 2: Data Cleaning and Preparation*</span>
```{r}

# Check for duplicates
duplicates <- cars.df[duplicated(cars.df), ]
print(paste("Number of duplicate entries:", nrow(duplicates)))

# Remove duplicates
cars.df <- cars.df[!duplicated(cars.df), ]


# The missing values and errors have been filled in the corresponding jupyter notebook. Hence, there is no nead for extra cleaning.


```

## <span style="color: blue">*Step 3: Visualizing the data*</span>

Some example questions we might be interested in:

- What is the relationship between car mileage and price?
- Which features  are most important for predicting car price?
- Which features (numerical and categorical) show the strongest correlation with car prices?


Distributions of used car variables
===


```{r cars_plots, echo=FALSE, fig.width=20, fig.height=30}  
par(mfrow = c(4, 3), mar = c(10, 6, 5, 2), oma = c(2, 2, 2, 2))  # Increased bottom margin for title space

# Loop through each column in cars.df
for (j in 1:ncol(cars.df)) {
  if (is.numeric(cars.df[, j])) {
    hist(cars.df[, j], 
         xlab = "",  
         main = paste("Histogram of", colnames(cars.df)[j]),
         col = "lightblue", 
         breaks = 50,
         cex.axis = 2,          # Increased axis text size
         cex.main = 2,          # Increased title text size
         cex.lab = 1.5)         # Increased label size for x/y axis
    
    # Place x-axis label at the bottom-right corner for histograms
    mtext(colnames(cars.df)[j], side = 1, line = 6, cex = 1.5, adj = 1)
    
  } else if (is.factor(cars.df[, j]) || is.character(cars.df[, j])) {
    # Focus on top 5 most common categories
    category_counts <- table(cars.df[, j])
    top_categories <- sort(category_counts, decreasing = TRUE)[1:5]
    
    barplot(top_categories, 
            main = paste("Bar Plot of", colnames(cars.df)[j]),
            xlab = NULL,  # Remove x-axis label for bar plots
            col = "lightgreen",
            las = 2,              # Rotate x-axis labels for better readability
            cex.names = 1.5,      # Increased size of category names
            cex.axis = 2,         # Increased axis text size
            cex.main = 2)         # Increased title text size
    
    # Adjust space between the plots
    box()  # Adds a box around the plot area
    mtext(colnames(cars.df)[j], side = 1, line = 7, cex = 1.5, adj = 1)  # Move x-axis label further up
  }
}

```

```{r }
# Scatter plot matrix for numeric variables in cars.df
pairs(~ price + highway_fuel_economy  + 
       mileage + seller_rating + year, 
      data = cars.df, 
      main = "Scatter Plot Matrix of Numeric Variables",
      pch = 19,          # Use filled circles for points
      col = "blue",      # Color of points
      cex = 0.5)         # Size of points



# Ensure libraries are loaded
library(ggplot2)

# Function to filter the top 5 categories
top5_categories <- function(data, column) {
  category_counts <- table(data[[column]])
  top_categories <- names(sort(category_counts, decreasing = TRUE)[1:5])
  data[data[[column]] %in% top_categories, ]
}

# Filter data for top 5 categories for each column
top_body_type <- top5_categories(cars.df, "body_type")
top_fuel_type <- top5_categories(cars.df, "fuel_type")
top_make_name <- top5_categories(cars.df, "make_name")

# Set larger margins and spacing between boxplots
par(mar = c(8, 6, 4, 2))  # Increase bottom margin for x-axis labels

# Plot 1: Price by Body Type
boxplot(price ~ factor(body_type, levels = unique(top_body_type$body_type)), 
        data = top_body_type,
        main = "Price by Body Type",
        xlab = "",
        ylab = "Price",
        col = "blue",
        las = 2,              # Rotate x-axis labels
        cex.axis = 0.8,       # Reduce size of axis text
        cex.main = 1.5,       # Larger title
        cex.lab = 1.2,        # Larger y-axis label
        outline = FALSE)      # Remove outliers for clarity
mtext("Body Type", side = 1, line = 6, cex = 1.2)  # Add x-axis label

# Plot 2: Price by Fuel Type
boxplot(price ~ factor(fuel_type, levels = unique(top_fuel_type$fuel_type)), 
        data = top_fuel_type,
        main = "Price by Fuel Type",
        xlab = "",
        ylab = "Price",
        col = "green",
        las = 2, 
        cex.axis = 0.8, 
        cex.main = 1.5, 
        cex.lab = 1.2,
        outline = FALSE) #  exclude outliers from the plot for better visualization
mtext("Fuel Type", side = 1, line = 6, cex = 1.2)

# Plot 3: Price by Make Name
boxplot(price ~ factor(make_name, levels = unique(top_make_name$make_name)), 
        data = top_make_name,
        main = "Price by Make Name",
        xlab = "",
        ylab = "Price",
        col = "red",
        las = 2, 
        cex.axis = 0.8, 
        cex.main = 1.5, 
        cex.lab = 1.2,
        outline = FALSE)
mtext("Make Name", side = 1, line = 6, cex = 1.2)



library(dplyr)
# Count the frequency of each car make
make_frequency <- cars.df %>%
  group_by(make_name) %>%
  summarise(frequency = n()) %>%
  arrange(desc(frequency))  # Sort in descending order

# Get the make with the largest frequency
largest_make <- make_frequency %>% slice(1)

# Print the result
cat("The most common manufacturer is:", largest_make$make_name, "with", largest_make$frequency, "vehicles.\n")


```

---

What can we observe from the plots?

Histograms and bar plots:

-`body_type`: The most common car types are SUV, Sedan and Pickup Truck with the order named.

-`fuel_type`: Overwhelmingly most cars burn gasoline.

-`has_accidents`: The majority of cars had no accidents in the past.

-`price`: The majority of cars cost under 50,000.

-`seller_rating`: The ratings of the cars mostly fluctuate around 3.5-4.5.

-`year`: The year of manufacture of most vehicles is after 2015.

-`make_name` : The manufacturer with the most cars is Ford

Scatter plots:

-Price vs. Mileage: The third plot in the first row shows that as mileage increases, prices tend to decrease, which is common as cars with higher mileage are often older and have more wear, leading to lower resale values.

-Price vs. Year: The last plot in the first row shows the relationship between price and year. Newer cars (with recent years) tend to have higher prices, which makes sense as cars generally depreciate over time.

-Mileage vs. Year: The plot in the third row and last column might show a trend where older cars have higher mileage, reflecting accumulated usage over time.

-Highway_fuel_economy vs Year: The plot in the second row and last columns indicates that the most recent manufactured cars tend to have better highway fuel economy. 

-Price vs fuel_type: Electric cars are more expensive (higher μ) due to high battery and technology costs, premium features, limited production scale, and positioning as luxury or flagship models.

-Price vs body_type: Pickup trucks are more expensive (higher μ) due to their larger size, powerful engines, heavy-duty construction for towing and hauling, and increasing demand for luxury and technology features in modern models.The cheapest (lower μ) is Hatchback.

-Price vs make_name: BMW is more expensive (higher μ) because it is positioned as a luxury brand, offering premium materials, advanced technology, performance-focused engineering, and a reputation for prestige and status compared to more mass-market brands like Jeep, Chevrolet (most affordable), Ford, and Toyota.

  
Correlations between used car variables
===
```{r}
# Load necessary library
library(dplyr)

# Separate numerical and categorical variables
numerical_vars <- cars.df %>% select(daysonmarket, highway_fuel_economy, horsepower, mileage, price, seller_rating, year)
categorical_vars <- cars.df %>% select(body_type, fuel_type, has_accidents, is_new, make_name)

# Calculate the correlation matrix for numerical variables
cars.cor = cor(numerical_vars, use = "complete.obs")  # Use 'complete.obs' to handle NA values
correlation_matrix = round(cars.cor, 3)

# Display the correlation matrix
print(correlation_matrix)
```


Some strong correlations! Let's find the biggest (in absolute value):

```{r}
cars.cor[lower.tri(cars.cor,diag=TRUE)] = 0 # Why only upper tri part?
#Setting this part to 0 means only the upper triangle (above the diagonal) retains the correlation values. This removes duplicate values (since correlations are symmetric) and eliminates self-correlations (diagonal elements), which are always 1.

cars.cor.sorted = sort(abs(cars.cor),decreasing=T)
cars.cor.sorted[1]
vars.big.cor = arrayInd(which(abs(cars.cor)==cars.cor.sorted[1]), 
                        dim(cars.cor)) # Note: arrayInd() is useful
colnames(numerical_vars)[vars.big.cor] 
```

Let's find the second biggest correlation (in absolute value):

```{r}
cars.cor.sorted[2]
vars.big.cor = arrayInd(which(abs(cars.cor)==cars.cor.sorted[2]), 
                        dim(cars.cor))
#arrayInd(...) then converts this linear index to row and column indices, using dim(pros.cor) to match the dimensions of pros.cor.
colnames(numerical_vars)[vars.big.cor] 
```



## <span style="color: blue">*Step 4: Perform Hypothesis testing using the initial inferences from the previous step*</span>

Let's state some questions based on our visualizations and check if they are valid or not.

1.Is SUV the most common `body_type`? Does `SUV/Crossover` type constitute more than 50% of the entire car data set? 

2.The wear and tear of the cars is a very significant parameter to assess the value of them. Let's examine if cars with has_accidents=True and Year<=2015 tend to have lower prices that cars with has_accidents=False and Year>2015.

3.Regarding BMW's expensive cars, one can assume that it has the cars with higher quality, hence higher seller ratings for the other most common cars manufacturers (i.e. Jeep., Chevrolet, Ford, Toyota). Let's examine if that states.

4.Does over 95% of cars have less than 150,000 kilometers? For the histogram of mileage one can observe that almost all cars have less than 150,000 kilometers.

In these cases, the code performs a t-test with the assumption of unequal variances (var.equal = FALSE), which is a key indicator that the population variances are unknown and not assumed to be the same. This makes the t-test more flexible in handling real-world data where these assumptions may not hold. The t-distribution also adjusts for the added uncertainty from estimating the population standard deviation.
```{r}
#1.
# Step 1: Create a binary variable for SUV
cars.df$suv_binary <- ifelse(cars.df$body_type == "SUV / Crossover", 1, 0)

# Step 2: Perform a one-sample t-test
# Testing if the mean frequency of SUVs is significantly greater than 0.5 (50%)
t.test(cars.df$suv_binary, mu = 0.5, alternative = "greater")
```
With a p-value of 0.0001171, this result is statistically significant at the 0.05 level, meaning there’s strong evidence to reject the null hypothesis. This indicates that the mean proportion of SUVs in the dataset is significantly greater than 0.5, or 50%.

```{r}
#2. 
# Filter data for Group 1 and Group 2
group1_prices <- cars.df$price[cars.df$has_accidents == "True" & cars.df$year <= 2015]
group2_prices <- cars.df$price[cars.df$has_accidents == "False" & cars.df$year > 2015]

# Perform Welch's t-test
t_test <- t.test(group1_prices, group2_prices, alternative = "less")

# Display results
t_test

```
The test provides strong evidence that cars with accidents (Group 1) and older manufacturing years (≤ 2015) tend to have significantly lower prices than cars with no accidents (Group 2) and recent manufacturing years (> 2015).

```{r}
#3
# Filter seller ratings by manufacturer
bmw_ratings <- cars.df$seller_rating[cars.df$make_name == "BMW"]
other_ratings <- cars.df$seller_rating[cars.df$make_name %in% c("Jeep", "Chevrolet", "Ford", "Toyota")]

# Compute the average of the other four manufacturers
average_other <- mean(other_ratings, na.rm = TRUE)

# Perform one-sample t-test
t_test <- t.test(bmw_ratings, mu = average_other, alternative = "greater")

# Display results
t_test

```
The test provides strong evidence that BMW’s average seller rating (4.11) s significantly higher than the average seller rating of the other four manufacturers combined (3.9966)

```{r}
#4.
# Step 1: Create a binary variable for cars with less than 100,000 kilometers
cars.df$less_than_100k <- ifelse(cars.df$mileage < 150000, 1, 0)

# Step 2: Perform a one-sample t-test
t_test_result <- t.test(cars.df$less_than_100k, mu = 0.95, alternative = "greater")

# Display the result
print(t_test_result)
```
This extremely low p-value suggests strong evidence against the null hypothesis. You can reject the null hypothesis, indicating that the mean of the binary variable (the proportion of cars with less than 150,000 kilometers) is significantly greater than 0.95.


## <span style="color: blue">*Step 5: Model Building step*</span>

The linear model is arguably the **most widely used** statistical model, has a place in nearly every application domain of statistics

Given response $Y$ and predictors $X_1,\ldots,X_p$, in a **linear regression model**, we posit:

$$
Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p + \epsilon, 
\quad \text{where $\epsilon \sim N(0,\sigma^2)$}
$$

```{r}
# Step 1: Calculate car age
cars.df$car_age <- 2024 - cars.df$year

# Step 2: Select categories and filter data
selected_categories <- c("Toyota Sedan", "Honda SUV / Crossover", "Ford Pickup Truck") # Update with actual category names
cars.df$category <- paste(cars.df$make_name, cars.df$body_type)
cars_subset <- subset(cars.df, category %in% selected_categories)

# Step 3: Build the linear model
lm_model <- lm(price ~ mileage + car_age, data = cars_subset)

# Step 4: Calculate residuals and add to the data frame
cars_subset$residuals <- resid(lm_model)

# Step 5: Plot residuals by category
boxplot(residuals ~ category, data = cars_subset, 
        main = "Residuals by Car Category",
        xlab = "Car Category",
        ylab = "Residuals",
        col = c("lightblue", "lightgreen", "lightpink"))
```

Interpretation:

The boxplot above displays the residuals of the linear regression model for each selected car category.

- `Ford Pickup Truck`: The median residual appears to be around 10,000. This indicates that, on average, the model overestimates the price of Ford Pickup Trucks by about 10,000 units.The interquartile range (IQR) is relatively wide, suggesting that the model's predictions for this category have a larger variability.

- `Honda SUV / Crossover`: The model's predictions are relatively accurate, with a median residual close to 0 and a narrower spread of residuals.

- `Toyota Sedans`: The median residual is around -10,000, indicating that the model tends to underestimate the price of Toyota Sedans by about 10,000 units. The IQR is also narrow, suggesting consistent predictions for this category.

## <span style="color: blue">*Step 6: Residual Analysis of Model*</span>

According to the logic of Open Intro Stat book, Section 8.2, I choose to use the Forward selection approach in order to identify the best subset of predictors for my model, based on adjusted \( R^2 \). The steps are the following:

1.I will start with a null model (no predictors), and calculate the adjusted \( R^2 \) for this baseline model. 

2.I will add each variable one at a time and check the adjusted \( R^2 \) for each resulting model.

3.I will add the variable that results in the highest adjusted \( R^2 \), provided it improves over the current model's adjusted \( R^2 \)

4.I will repeat by adding additional variables one at a time, selecting the variable that offers the highest adjusted \( R^2 \) improvement until no significant improvement is achieved.

```{r}

# Step 1: Define a null model and a full model
null_model <- lm(price ~ 1, data = cars.df) # model with no predictors
full_model <- lm(price ~ mileage: + car_age + horsepower , data = cars.df) # model with all predictors, note that I chose as predictors only variables that have absolut correlation with price > 0.5

# Step 2: Use stepwise selection with forward direction based on adjusted R-squared
forward_model <- step(null_model, 
                      scope = list(lower = null_model, upper = full_model), 
                      direction = "forward", 
                      k = log(nrow(cars.df))) # k = log(n) is used for BIC(Bayesian Information Criterion), which balances fit and complexity

# Print the summary of the forward selection model
summary(forward_model)

```
Interpretation

- Residual Standard Error (RSE): This value of 17440 indicates that, on average, the model's predictions for car prices are off by about $17,440. This suggests that the model's predictions are relatively inacurate.
- Multiple R-squared: 0.4256. This value indicates that approximately 42.56% of the variation in car prices can be explained by the model, which includes horsepower and mileage as predictors. This suggests a moderate level of fit.
- Adjusted R-squared: 0.4256.  This value is same as the R-squared, indicating that the model's fit is not significantly affected by the number of predictors.
- F-statistic and p-value: The extremely low p-value (< 2.2e-16) suggests that the overall model is highly statistically significant, meaning that at least one of the predictors (horsepower or mileage) is significantly related to car price.
- Coefficient Interpretation:
  - Horsepower: A one-unit increase in horsepower is associated with a $135.3 increase in car price, on average, holding mileage constant.
  - Mileage: A one-unit increase in mileage is associated with a $0.21 decrease in car price, on average, holding horsepower constant.

Residual Analysis
===

```{r}
# Plot residuals to check assumptions

plot(forward_model)
```
Results:

- `Resuduals vs Fitted`: The Residuals vs Fitted plot shows a non-random pattern, indicating that the model assumptions are not met. There appears to be a funnel shape, suggesting that the variance of the residuals increases as the fitted values increase. This violates the assumption of homoscedasticity, where the variance of the residuals should be constant across all fitted values. Additionally, there are a few outliers that deviate significantly from the main pattern.

- `Q-Q residuals`: The deviation from normality, especially in the upper tail, suggests that the model may not fully meet the assumptions of linear regression, potentially affecting the validity of inference statistics such as confidence intervals and p-values.

-  `Scale-Location`: The Scale-Location plot shows a non-random pattern in the standardized residuals, indicating that the model assumptions are not met. There appears to be a U-shaped pattern, suggesting that the spread of the residuals changes as the fitted values increase and then decrease. This violates the assumption of homoscedasticity, where the variance of the residuals should be constant across all fitted values. This pattern suggests that the model may not be equally reliable for all ranges of fitted values.

- `Residuals vs Leverage`: The Residuals vs Leverage plot helps identify influential points in the data. These are points that have a significant impact on the model's fit and predictions. The plot shows three influential points labeled "28721", "17172", and "28460". These points have high leverage, meaning they have a large impact on the model's estimates. Additionally, they have relatively large standardized residuals, indicating that they are not well-explained by the model. These points could be outliers or data points with unique characteristics that the model is not able to capture. 